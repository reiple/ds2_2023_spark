{"cells":[{"cell_type":"markdown","metadata":{"id":"zaCjW3u7-3Rr"},"source":["# Setting Pyspark in Colab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xy21kj6l-sVM"},"outputs":[],"source":["!wget -q https://archive.apache.org/dist/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n","!tar xf spark-3.2.4-bin-hadoop3.2.tgz\n","!pip install -q findspark"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NuSXGIaf-_cL"},"outputs":[],"source":["import findspark\n","findspark.init(\"/content/spark-3.2.4-bin-hadoop3.2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wsPoOtSy_BuF"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","sc = spark.sparkContext\n","sc"]},{"cell_type":"markdown","metadata":{"id":"TI8T1v1BFAjT"},"source":["# RDD Exercise - Word Count (gutenberg)\n","\n","1. gutenberg.txt 파일 로드\n","2. .map() 을 사용하여 주어진 데이터의 문장부호를 제거하고 소문자로 변환  (python 함수 .replace(), .lower() )\n","3. .flatmap() 을 사용하여 주어진 데이터를 단어 단위로 구분\n","4. .map() 을 사용하여 주어진 데이터로 (word, 1) tuple 생성\n","5. .reduceByKey() 를 사용하여 word를 기준으로 결합\n","6. 상위 10개의 결과 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Pwd0QpLE_ul"},"outputs":[],"source":["wordcount = sc.textFile(\"gutenberg.txt\") \\\n","              .map(lambda x: x.replace(\",\", \"\").replace(\".\", \"\").replace(\"'\", \"\").replace('\"', \"\").lower()) \\\n","              .flatMap(lambda x: x.split()) \\\n","              .map(lambda x: (x, 1)) \\\n","              .reduceByKey(lambda x, y: x + y) \\\n","              .map(lambda x: (x[1], x[0])) \\\n","              .sortByKey(False)\n","\n","print(wordcount.take(10))"]},{"cell_type":"markdown","metadata":{"id":"Os9jrit0XeIb"},"source":["## 각 단계의 처리 결과 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cXDWUvS_Xc37"},"outputs":[],"source":["rdd1 = sc.textFile(\"gutenberg.txt\")  # 파일로부터 rdd 만들기\n","rdd1.take(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eHT1urp9XtMg"},"outputs":[],"source":["rdd2 = rdd1.map(lambda x: x.replace(\",\", \"\").replace(\".\", \"\").replace(\"'\", \"\").replace('\"', \"\").lower())  # 문장부호 제거 및 소문자로\n","rdd2.take(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFdkM75AX0tQ"},"outputs":[],"source":["rdd3 = rdd2.flatMap(lambda x: x.split())  # 단어 단위 분리 (map이 아닌 flatMap을 사용한다는 점에 주목)\n","rdd3.take(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q3LoZV8RX_sI"},"outputs":[],"source":["rdd4 = rdd3.map(lambda x: (x, 1))  # reduceByKey를 사용하기 위해 (key, value)의 튜플 형태로 만듦. key=단어, value=1\n","rdd4.take(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HZsdcJN6YMzj"},"outputs":[],"source":["rdd5 = rdd4.reduceByKey(lambda x, y: x + y)  # reduceByKey를 사용하여 같은 key(단어)의 value(1)들을 모두 더함. 즉, 단어들의 등장 횟수를 구함\n","rdd5.take(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LxWn3Y6hYeEy"},"outputs":[],"source":["rdd6 = rdd5.map(lambda x: (x[1], x[0]))  # sortByKey를 사용하기 위해 (key, value)의 튜플 형태로 만듦. key=단어 등장 빈도, value=단어\n","rdd6.take(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tZkmXhoqYqJg"},"outputs":[],"source":["rdd7 = rdd6.sortByKey(False)  # sortByKey를 사용하여 key 크기 순으로 정렬. 인자로 False를 주면 내림차순 정렬, 인자로 True를 주면 오름차순 정렬.\n","rdd7.take(10)  # 최빈 단어 10개 출력"]},{"cell_type":"markdown","metadata":{"id":"NTbNyMFGzada"},"source":["# Spark DataFrame Exercise - Titanic Dataset"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/gdrive')"],"metadata":{"id":"aJGwdPL0r7f1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터프레임 생성\n","titanic_df = spark.read.option(\"header\", \"true\") \\\n","                  .option(\"nullValue\", \"?\") \\\n","                  .option(\"inferSchema\", \"true\") \\\n","                  .csv('/gdrive/MyDrive/titanic_train.csv')\n","print('titanic 변수 type:',type(titanic_df))"],"metadata":{"id":"fOG2dysRzRkN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터프레임 자료형 보기 1\n","titanic_df"],"metadata":{"id":"nKvqJM2D0Ere"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터프레임 자료형 보기 2\n","titanic_df.printSchema()"],"metadata":{"id":"X6cC9hRM0nQk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터프레임 자료형 보기 3\n","titanic_df.dtypes"],"metadata":{"id":"KCvy06JF6Z8x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터프레임 크기 보기\n","print('DataFrame 크기: ', titanic_df.count(),'rows, ', len(titanic_df.columns),'columns')"],"metadata":{"id":"9HpJnlTO0k_U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터프레임 내용 보기\n","titanic_df.show(3)"],"metadata":{"id":"pHT7fHiZ0eXr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터프레임 요약 보기\n","titanic_df.describe().show()"],"metadata":{"id":"N_NqTvYT0q8t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터프레임 열 선택 1 : subset\n","titanic_df[\"PassengerId\"].dtypes"],"metadata":{"id":"tBK43cvj0sIc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터프레임 열 선택 2 : attribute\n","titanic_df.PassengerId.dtypes"],"metadata":{"id":"o5jVFtVm6S7M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터프레임 열 선택 3 : select\n","titanic_pclass = titanic_df.select(\"Pclass\")\n","print(type(titanic_pclass)) # 데이터 타입이 Column이 아닌 DataFrame임에 주목\n","titanic_pclass.show(5)"],"metadata":{"id":"IkYtlBfD6XVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터프레임 열 생성 1 : lit() 이용\n","from pyspark.sql.functions import lit\n","titanic_df = titanic_df.withColumn(\"Age_0\", lit(0))\n","titanic_df.show(3)\n","\n","# withColumn(x, y) : x=Column 이름, y=Column 객체\n","# lit() : 전달된 값으로 채워진 Column 객체를 반환\n","# ex) lit(0) : 0으로 채워진 Column 객체를 반환\n","# 오직 리터럴(literal)만 입력으로 받을 수 있다\n","# 리터럴의 예) 숫자(1, 2, 3), 문자열(\"asdf\", \"hello\") 등\n","# 불가능한 입력의 예) 리스트([1, 2]), 튜플((1, 2)) 등"],"metadata":{"id":"m3DYekA883jn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터프레임 열 생성 2 : 다른 Column으로부터\n","from pyspark.sql.functions import col\n","titanic_df = titanic_df.withColumn(\"Age_by_10\", col(\"Age\") * 10)\n","titanic_df = titanic_df.withColumn(\"Family_No\", col(\"SibSp\") + col(\"Parch\") + 1)\n","titanic_df.show(3)\n","\n","# col() : 전달된 값의 이름을 가진 Column 객체를 반환"],"metadata":{"id":"G4Lc5A30_YxV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터프레임 열 수정\n","from pyspark.sql.functions import col\n","titanic_df = titanic_df.withColumn(\"Age_by_10\", col(\"Age_by_10\") + 100)  # 덮어씌워짐\n","titanic_df.show(3)"],"metadata":{"id":"Mc4dqvGv_7mO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터프레임 열 삭제\n","titanic_df = titanic_df.drop(\"Age_0\")\n","titanic_df.show(3)"],"metadata":{"id":"knkcXLh_AF3a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# drop 명령어는 원본을 수정하지 않음\n","drop_result = titanic_df.drop('Age_0', 'Age_by_10', 'Family_No')\n","print('drop 후 반환된 값:', type(drop_result))\n","titanic_df.show(3)\n","drop_result.show(3)\n","\n","titanic_df = drop_result"],"metadata":{"id":"X2Ia3KhAATAi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터프레임 조건부 선택 : filter, where\n","titanic_df.where(titanic_df.Pclass == 3).show(3)\n","titanic_df.where(titanic_df['Pclass'] == 3).show(3)\n","titanic_df.filter(titanic_df.Pclass == 3).show(3)\n","titanic_df.filter(titanic_df['Pclass'] == 3).show(3)"],"metadata":{"id":"7jPc0fC8JtW0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터프레임 조건부 선택 : Boolean Indexing\n","titanic_boolean = titanic_df.filter(titanic_df['Age'] > 60).show(3)\n","\n","titanic_df.filter(titanic_df['Age'] > 60).select('Name','Age').show(3)\n","\n","titanic_df.filter((titanic_df['Age'] > 60) & (titanic_df['Pclass'] == 1) & (titanic_df['Sex'] == 'female')).show()\n","\n","cond1 = titanic_df['Age'] > 60\n","cond2 = titanic_df['Pclass'] == 1\n","cond3 = titanic_df['Sex'] == 'female'\n","titanic_df.filter(cond1 & cond2 & cond3).show()"],"metadata":{"id":"EvZ80nJ3KQRb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터프레임 정렬\n","titanic_df.sort('Name').show(3)\n","\n","titanic_df.sort(['Pclass', \"Name\"], ascending=[False, False]).show(3)"],"metadata":{"id":"9cPkayKzLXze"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터프레임 묶기(groupby)\n","titanic_groupby = titanic_df.groupby('Pclass')\n","titanic_groupby.mean().show()\n","\n","titanic_df.groupBy(\"Pclass\").count().show()"],"metadata":{"id":"dNQpY7gd63jb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Groupby Aggregation\n","from pyspark.sql.functions import max, min, sum, avg, count\n","\n","titanic_df.groupby('Pclass').agg(max('Age'), min('Age')).sort('Pclass').show()\n","\n","exprs = [count('Age'), sum('SibSp'), avg('Fare')]\n","titanic_df.groupby('Pclass').agg(*exprs).sort('Pclass').show()\n","\n","exprs = [count(x).alias(x) for x in titanic_df.columns]\n","titanic_groupby = titanic_df.groupby('Pclass').agg(*exprs)\n","titanic_groupby.show()\n","# NULL 값은 세어지지 않음\n","# cf) titanic_df.filter(titanic_df[\"Pclass\"] == 1).filter(\"Age is not NULL\").count() == 186\n","# cf) titanic_df.filter(titanic_df[\"Pclass\"] == 3).filter(\"Cabin is not NULL\").count() == 12"],"metadata":{"id":"RyISzDhzT001"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aggregation\n","titanic_df.agg(count(col('PassengerId')), count(col('Age')), count(col('Cabin'))).show()\n","\n","exprs = [count(x) for x in titanic_df.columns]\n","titanic_df.agg(*exprs).show()\n","\n","titanic_df.agg(avg(col('Age')),avg(col('Fare'))).show()"],"metadata":{"id":"_XeEVibFMBYp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 결손 데이터\n","from pyspark.sql.functions import when\n","\n","exprs = [(col(x).isNull()).alias(x) for x in titanic_df.columns]\n","titanic_df.select(*exprs).show(3)\n","\n","titanic_df.select([count(when(col(x).isNull(), x)).alias(x) for x in titanic_df.columns]).show(3)"],"metadata":{"id":"KNGkA-7ML89-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 결손 데이터 채우기\n","titanic_df = titanic_df.fillna({'Cabin':'C000'})\n","titanic_df.show(3)\n","\n","Age_mean = titanic_df.agg(avg('Age').alias('Age')).first().asDict()\n","print(Age_mean)\n","titanic_df = titanic_df.fillna(Age_mean) # 평균값으로 채우기\n","titanic_df = titanic_df.fillna({'Embarked':'S'})\n","titanic_df.select([count(when(col(x).isNull(), x)).alias(x) for x in titanic_df.columns]).show(3)"],"metadata":{"id":"8T2kU75jTa7R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kkyLrsXLhoeg"},"source":["# 기타"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M0uNqI_Zgtm0"},"outputs":[],"source":["# join\n","df1 = sc.parallelize([[\"Michael\", None], [\"Andy\", 30], [\"Justin\", 19]]).toDF(['name', 'age'])\n","df2 = sc.parallelize([[\"Michael\", 2000], [\"Andy\", 2500], [\"Justin\", 5000], [\"Bob\", 6500]]).toDF(['name', 'income'])\n","\n","df3 = df1.join(df2, on=\"name\", how=\"inner\")\n","df3.show()\n","\n","df4 = df1.join(df2, on=\"name\", how=\"outer\")\n","df4.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6jat0cQMhn9x"},"outputs":[],"source":["# SQL\n","df = sc.parallelize([[\"Michael\", None], [\"Andy\", 30], [\"Justin\", 19]]).toDF(['name', 'age'])\n","\n","df.createOrReplaceTempView(\"people\")\n","sqlDF = spark.sql(\"SELECT * from people\")\n","sqlDF.show()"]},{"cell_type":"markdown","metadata":{"id":"tDoI3qr8gaLt"},"source":["## Lambda 함수"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UzZvsMiift5t"},"outputs":[],"source":["def get_square(a):\n","    return a**2\n","print('3의 제곱은:',get_square(3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V0Bjg0GVf1UJ"},"outputs":[],"source":["lambda_square = lambda x : x**2\n","print('3의 제곱은:',lambda_square(3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f-c-XkF8f8-5"},"outputs":[],"source":["a=[1,2,3]\n","square = map(lambda x: x**2, a)\n","list(square)"]},{"cell_type":"markdown","metadata":{"id":"3NwZ82eqiFqL"},"source":["## udf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a6ypLaT2iSpY"},"outputs":[],"source":["from pyspark.sql.functions import udf\n","from pyspark.sql.types import *\n","\n","slen = udf(lambda s: len(s), IntegerType())\n","\n","@udf\n","def to_upper(s):\n","  if s is not None:\n","    return s.upper()\n","\n","@udf(returnType=IntegerType())\n","def add_one(x):\n","  if x is not None:\n","    return x + 1\n","\n","df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n","df.select(slen(\"name\").alias(\"length of name\"), to_upper(\"name\"), add_one(\"age\")).show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wJcXVFAfgf_T"},"outputs":[],"source":["# CHild Adult 열 생성\n","from pyspark.sql.functions import udf\n","from pyspark.sql.types import *\n","\n","get_age_cat = udf(lambda x: 'Child' if x<=15 else 'Adult', StringType())\n","titanic_df = titanic_df.withColumn('Child_Adult', get_age_cat(titanic_df['Age']))\n","titanic_df.select('Age','Child_Adult').show(8)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xwc1YLh3iKSM"},"outputs":[],"source":["# Age Category 열 생성\n","lambda_age_cat = lambda x: 'Child' if x<=15 \\\n","                                  else ('Adult' if x <= 60 else 'Elderly')\n","get_age_cat = udf(lambda_age_cat, StringType())\n","titanic_df = titanic_df.withColumn('Age_cat', get_age_cat(titanic_df['Age']))\n","titanic_df.groupby('Age_cat').count().show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CDEgZf-2jEoA"},"outputs":[],"source":["def get_category(age):\n","  cat = ''\n","  if age <= 5:\n","    cat = 'Baby'\n","  elif age <= 12:\n","    cat = 'Child'\n","  elif age <= 18:\n","    cat = 'Teenager'\n","  elif age <= 25:\n","    cat = 'Student'\n","  elif age <= 35:\n","    cat = 'Young Adult'\n","  elif age <= 50:\n","    cat = 'Adult'\n","  else:\n","    cat = 'Elderly'\n","  return cat\n","\n","udf_get_category = udf(get_category, StringType())\n","titanic_df = titanic_df.withColumn('Age_cat', udf_get_category(titanic_df['Age']))\n","titanic_df.select('Age','Age_cat').show(5)"]},{"cell_type":"markdown","metadata":{"id":"skNEFvYOjx7W"},"source":["# 따릉이 데이터"]},{"cell_type":"code","source":["# 대여소 데이터 불러오기\n","df_loc = spark.read\\\n","              .option(\"header\",\"true\")\\\n","              .option(\"inferSchema\",\"true\")\\\n","              .csv('/gdrive/MyDrive/Colab Notebooks/bikeshare_loc.csv')\n","df_loc.show(10)"],"metadata":{"id":"bwgQjaLmIkAC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 일간 대여/반납 데이터 불러오기\n","df_usage = spark.read\\\n","                .option(\"header\",\"true\")\\\n","                .option(\"inferSchema\",\"true\")\\\n","                .csv('/gdrive/MyDrive/Colab Notebooks/bikeshare_seoul.csv')\n","df_usage.show(10)"],"metadata":{"id":"VcbWwn-BIkAD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 따릉이 대여소 번호 2319의 정보는?"],"metadata":{"id":"aomzqFYtIkAD"}},{"cell_type":"code","source":["result_df = \"fill here\"  # df_loc에서 \"대여소번호\" column의 값이 2319인 행을 필터\n","\n","result_df.show()"],"metadata":{"id":"9mHAMgD2IkAD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 따릉이 대여소는 어느 구에 가장 많이 있을까?"],"metadata":{"id":"VRLL2rH_IkAE"}},{"cell_type":"code","source":["result_grouped = \"fill here\"  # df_loc dataframe을 '구명' column으로 grouping한다\n","\n","result_grouped_counted = result_grouped.\"fill here\"  # 각 그룹별로 몇 개의 항목이 있는지 count한다.\n","\n","result_grouped_counted_sorted = result_grouped_counted.\"fill here\"  # count column의 내림차순으로 정렬\n","\n","result_grouped_counted_sorted.show()"],"metadata":{"id":"ONu9XCR4kZE-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dcbYkFH3j1_9"},"outputs":[],"source":["# 일간 대여/반납 데이터 불러오기\n","df_usage = spark.read\\\n","                .option(\"header\",\"true\")\\\n","                .option(\"inferSchema\",\"true\")\\\n","                .csv('/gdrive/MyDrive/Colab Notebooks/bikeshare_seoul.csv')\n","\n","df_usage.show(10)"]},{"cell_type":"markdown","metadata":{"id":"eaFzK0o3kXmj"},"source":["## 따릉이 대여소 번호 2319의 정보는?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z6vN3-BikE8H"},"outputs":[],"source":["result_df = df_loc.filter(df_loc['대여소번호'] == 2319)  # df_loc에서 \"대여소번호\" column의 값이 2319인 행을 필터\n","\n","result_df.show()"]},{"cell_type":"markdown","metadata":{"id":"Hhk_0qGLkaHt"},"source":["## 따릉이 대여소는 어느 구에 가장 많이 있을까?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5kQReCJLL3Wx"},"outputs":[],"source":["result_grouped = df_loc.groupBy('구명')  # df_loc dataframe을 '구명' column으로 grouping한다\n","\n","result_grouped_counted = result_grouped.count()  # 각 그룹별로 몇 개의 항목이 있는지 count한다.\n","\n","result_grouped_counted_sorted = result_grouped_counted.sort(\"count\",ascending=False)  # count column의 내림차순으로 정렬\n","\n","result_grouped_counted_sorted.show()"]},{"cell_type":"markdown","metadata":{"id":"OOl55mlWki-Z"},"source":["## 각 구의 따릉이 대여소 수를 Bar Graph로 그려보자"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lDz0GoM7kxKC"},"outputs":[],"source":["!pip install -q koreanize-matplotlib\n","\n","import koreanize_matplotlib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cQ_1QJivkfuT"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jG9uB6t1kl0m"},"outputs":[],"source":["list_pd = df_loc.select('구명').toPandas()  # df_loc dataframe에서 \"구명\" column만 select한 후, pandas dataframe으로 변경한다.\n","\n","sns.countplot(x='구명', data=list_pd)  # sns.countplot : data 인자를 x 인자 column으로 grouping해 각 그룹에 몇 개의 항목이 있는지 그래프를 그린다.\n","\n","plt.xticks(rotation=90)  # x축 눈금들을 90도 회전해서 출력한다.\n","\n","plt.show()"]},{"cell_type":"markdown","source":["## 따릉이 거치대가 가장 많은 구는 어디일까?"],"metadata":{"id":"iQB1qM5dmWb1"}},{"cell_type":"code","source":["result_grouped = \"fill here\"  # df_loc dataframe을 '구명' column으로 grouping한다\n","\n","result_grouped_sum = result_grouped.\"fill here\"  # 같은 \"구명\" column을 가진 행들의 \"거치대수\" column들을 모두 더한다.\n","\n","result_grouped_sum_sorted = result_grouped_sum.\"fill here\"  # 덧셈 결과에 따라 내림차순 정렬한다.\n","\n","result_grouped_sum_sorted.show(1)"],"metadata":{"id":"GIj8MaPBknRX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 각 구의 따릉이 거치대 수를 Bar Graph로 그려보자"],"metadata":{"id":"skm-22-7mkGk"}},{"cell_type":"code","source":["result = \"fill here\".toPandas()  # df_loc dataframe을 \"구명\" column으로 grouping하고, 같은 \"구명\" column을 가진 항목끼리 \"거치대수\" column을 모두 더한 후, pandas dataframe으로 변환한다.\n","\n","sns.barplot(x=\"구명\", y=\"sum(거치대수)\", data=result)  # sns.barplot : data 인자의 x 인자 column과 y 인자 column 간의 막대그래프를 그린다.\n","plt.xticks(rotation=90)  # x축 눈금들을 90도 회전해서 출력한다.\n","plt.show()"],"metadata":{"id":"1ZnjQn6pmjkM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 강남구 2318번 따릉이 대여소의 2018년 5월 일간 따릉이 대여건수 추이는?"],"metadata":{"id":"jMS5QJPwmwU9"}},{"cell_type":"code","source":["from pyspark.sql.functions import col, split\n","\n","U2318_may = df_usage.where(\"fill here\")  # df_usage dataframe의 \"기준일자\" column이 2018년 5월 1일 ~ 2018년 5월 31일 사이에 있는 항목들만 선택한다.\n","U2318_may = U2318_may.select(\"fill here\")  # \"기준일자\", \"대여건수\", \"반납건수\", \"대여소번호\" column만 선택한다. 이때 \"대여소번호\" column은 \"대여소명\" column을 \".\"으로 split했을 때 가장 앞에 나오는 항목을 가져와서 만든다.\n","U2318_may = U2318_may.filter(col('대여소번호') == 2318)  # \"대여소번호\" column이 2318인 항목들만 선택한다.\n","\n","U2318_may.show(40)"],"metadata":{"id":"gCzQ9GO-mpqw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(16, 8))  # 그래프 크기 설정\n","sns.lineplot(x='기준일자', y='대여건수', data=U2318_may.toPandas())  # U2318_may dataframe을 pandas dataframe으로 변경하여 꺾은선그래프를 그린다.\n","plt.xticks(rotation=90)  # x축 눈금들을 90도 회전해서 출력한다.\n","plt.show()"],"metadata":{"id":"W0SwaLn3m5xQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2018년 중 일간 최고 대여건수를 기록한 대여소는?"],"metadata":{"id":"ODPz51AMoW05"}},{"cell_type":"code","source":["daily_best_df = df_usage.\"fill here\"  # df_usage dataframe에서 \"기준일자\" column이 2018년 01월 01일 ~ 2018년 12월 31일 사이에 있는 항목을 선택한다.\n","\n","daily_best_df = daily_best_df.\"fill here\"  # \"대여소위치\", \"대여소명\", \"기준일자\", \"대여건수\" column만 선택한다.\n","\n","daily_best_df = daily_best_df.\"fill here\"  # \"대여건수\" column에 대해 내림차순 정렬한다.\n","\n","daily_best_df.show(1)"],"metadata":{"id":"PXlOPK2xopNK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2018년 중 일간 최고 반납건수를 기록한 대여소는?"],"metadata":{"id":"ZhoimtQSoarr"}},{"cell_type":"code","source":["# \"2018년 중 일간 최고 대여건수를 기록한 대여소는?\" 항목을 참조해서 직접 해 봅시다.\n","\"fill here\""],"metadata":{"id":"yGBUqsoBo1fa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2018년 월별 강남구의 따릉이 대여소의 월 누적 대여/반납건수의 추이는?"],"metadata":{"id":"NSNDLAO_odpD"}},{"cell_type":"code","source":["import pyspark.sql.functions as sf\n","\n","#누적 대여건수\n","U2018_g_d = df_usage.\"fill here\"  # df_usage dataframe의 \"대여소위치\", \"대여건수\", \"월\" column만 선택한다. 이때 \"월\" column은 \"기준일자\" column으로부터 만든다(\"abcd-ef-gh\" 형태에서 가운데 \"ef\" 부분만 가져온다).\n","U2018_g_d = U2018_g_d.\"fill here\"  # \"대여소위치\" column을 이용해 grouping한다.\n","U2018_g_d = U2018_g_d.pivot('월').agg(sf.sum('대여건수'))  # (grouping된 column인) \"대여소위치\"를 행으로, (pivot() 메소드에 인자루 주어진 column인) \"월\"을 열로 하는 pivot table을 만들고, 각 셀의 값으로 해당 \"대여소위치\"와 \"월\"에 해당하는 항목들의 \"대여건수\"를 모두 더한 값을 채워 새로운 dataframe을 만든다.\n","U2018_g_d = U2018_g_d.\"fill here\"  # \"대여소위치\" column이 \"강남구\"인 column들만 선택한다.\n","U2018_g_d.show()\n","\n","#누적 반납건수 : 위 \"누적 대여건수\" 항목을 참조해서 직접 해 봅시다.\n","U2018_g_b = \"fill here\"\n","U2018_g_b.show()"],"metadata":{"id":"13V4j6pSo88-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from pandas.plotting import register_matplotlib_converters\n","\n","U2018_g_d_pd = U2018_g_d.toPandas().transpose()[1:].reset_index()  # 만들어진 U2018_g_d dataframe을 pandas dataframe으로 바꾼다.\n","U2018_g_d_pd.columns = ['월', '누적대여건수']  # column명을 \"월\", \"누적대여건수\"로 바꾼다.\n","U2018_g_d_pd = U2018_g_d_pd.apply(pd.to_numeric)  # 각 column들을 숫자 데이터 타입으로 바꾼다.\n","\n","U2018_g_b_pd = U2018_g_b.toPandas().transpose()[1:].reset_index()  # 만들어진 U2018_g_b dataframe을 pandas dataframe으로 바꾼다.\n","U2018_g_b_pd.columns = ['월', '누적반납건수']  # column명을 \"월\", \"누적대여건수\"로 바꾼다.\n","U2018_g_b_pd = U2018_g_b_pd.apply(pd.to_numeric)  # 각 column들을 숫자 데이터 타입으로 바꾼다."],"metadata":{"id":"7l9KjRaFpJSx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(16, 8))  # 그래프 크기 설정\n","sns.lineplot(x='월', y = '누적대여건수', data = U2018_g_d_pd)  # U2018_g_d_pd pandas dataframe의 꺾은선그래프를 그린다.\n","plt.show()"],"metadata":{"id":"vYe2BMuOpKdj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(16, 8))  # 그래프 크기 설정\n","sns.lineplot(x='월', y = '누적반납건수', data = U2018_g_b_pd)  # U2018_g_b_pd pandas dataframe의 꺾은선그래프를 그린다.\n","plt.show()"],"metadata":{"id":"VOMNCTQppSJ7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2018년 일 평균 대여건수가 가장 큰 대여소는?"],"metadata":{"id":"XHpSXYZ-ohSf"}},{"cell_type":"code","source":["result = \"fill here\"  # df_usage dataframe의 \"기준일자\" column의 값이 2018년 01월 01일 ~ 2018년 12월 31일 사이에 있는 값들만 선택한다.\n","\n","result = result.\"fill here\"  # \"대여소명\" column으로 grouping한다.\n","\n","result = result.\"fill here\"  # \"대여소명\"이 같은 항목들에 대해 \"대여건수\" column의 평균을 구해 \"평균대여건수\"라는 이름의 column으로 만든다.\n","\n","result = result.\"fill here\"  # \"평균대여건수\" column에 대해 내림차순 정렬한다.\n","\n","result.show(1)"],"metadata":{"id":"MFHZi_NpnBff"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2018년 누적 대여건수가 가장 많은 대여소는?"],"metadata":{"id":"D42XhnM0pcKP"}},{"cell_type":"code","source":["result = \"fill here\"  # df_usage dataframe의 \"기준일자\" column의 값이 2018년 01월 01일 ~ 2018년 12월 31일 사이에 있는 값들만 선택한다.\n","\n","result = result.\"fill here\"  # \"대여소명\" column으로 grouping한다.\n","\n","result = result.\"fill here\"  # \"대여소명\"이 같은 항목들에 대해 \"대여건수\" column의 합계을 구해 \"총대여건수\"라는 이름의 column으로 만든다.\n","\n","result = result.\"fill here\"  # \"총대여건수\" column에 대해 내림차순 정렬한다.\n","\n","result.show(1)"],"metadata":{"id":"okXJz-xSpYOR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2018년 누적 반납건수가 가장 많은 대여소는?"],"metadata":{"id":"9hEYMlUuptNz"}},{"cell_type":"code","source":["# 위 \"2018년 누적 대여건수가 가장 많은 대여소는?\" 항목을 참조해서 직접 해 봅시다.\n","\"fill here\""],"metadata":{"id":"Qv0I2Hc-p6tu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2018년 누적 대여건수가 가장 많은 지역구는?"],"metadata":{"id":"gFAzas7gptLS"}},{"cell_type":"code","source":["result = \"fill here\"  # df_usage dataframe의 \"기준일자\" column의 값이 2018년 01월 01일 ~ 2018년 12월 31일 사이에 있는 값들만 선택한다.\n","\n","result = result.\"fill here\"  # \"대여소위치\" column으로 grouping한다.\n","\n","result = result.\"fill here\"  # \"대여소위치\"가 같은 항목들에 대해 \"대여건수\" column들의 합을 구해 \"총대여건수\"라는 이름의 column으로 만든다.\n","\n","result = result.\"fill here\"  # \"총대여건수\" column에 대해 내림차순 정렬한다.\n","\n","result.show(1)"],"metadata":{"id":"7pzTysBNqAB4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2018년 누적 대여건수와 반납건수의 차이가 가장 큰 대여소/지역구는?"],"metadata":{"id":"Q9ZdTeBMptIi"}},{"cell_type":"code","source":["result = \"fill here\"  # df_usage dataframe의 \"기준일자\" column의 값이 2018년 01월 01일 ~ 2018년 12월 31일 사이에 있는 값들만 선택한다.\n","\n","result = result.\"fill here\"  # \"대여건수\" column과 \"반납건수\" column의 차를 계산해 \"대여차이\" column을 만든다.\n","\n","result = result.\"fill here\"  # \"대여소위치\" column으로 grouping한다.\n","\n","result = result.\"fill here\"  # \"대여소위치\"가 같은 gkdahremfdp eogo \"대여차이\" column의 합을 구해 \"총대여차이\"라는 이름의 column을 만든다.\n","\n","result = result.\"fill here\"  # \"총대여차이\" column으로 내림차순 정렬한다.\n","\n","result.show(1)"],"metadata":{"id":"X3E98WceqPTk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 대여건수가 100건 이상을 기록한 적이 있는 따릉이 대여소 중 가장 남쪽에 있는 대여소는?"],"metadata":{"id":"wOjZHmePptEK"}},{"cell_type":"code","source":["usage100 = \"fill here\"  # df_usage dataframe에서 \"대여건수\" column의 값이 100 이상인 항목들만 선택한다.\n","\n","usage100 = usage100.\"fill here\"  # \"대여건수\", \"반납건수\", \"대여소번호\" column을 선택한다. 이때 \"대여소번호\" column은 \"대여소명\" column을 \".\"으로 split했을 때 가장 앞에 나오는 항목을 가져와서 만든다.\n","\n","usage100.show(10)"],"metadata":{"id":"QpqcKFb8qeMo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_loc_selected = \"fill here\"  # df_loc dataframe에서 \"대여소번호\", \"구명\", \"대여소명\", \"위도\" column만 선택한다.\n","\n","df_joined = usage100.\"fill here\"  # 위에서 만든 usage100 dataframe과 df_loc_selected dataframe을 \"대여소번호\" column에 대해 inner join한다.\n","\n","df_joined = df_joined.\"fill here\"  # \"위도\"에 대해 오름차순 정렬한다.\n","\n","df_joined.show(1)"],"metadata":{"id":"QHn7oKUTqi_T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 따릉이 거치대 수 대비 따릉이 대여/반납건수 비율이 가장 높은 대여소는?"],"metadata":{"id":"wqLF756cps1p"}},{"cell_type":"code","source":["df_usage_selected = \"fill here\"  # df_usage dataframe에서 \"대여소위치\", \"대여건수\", \"반납건수\", \"대여소번호\" column을 선택한다. 이때 \"대여소번호\" column은 \"대여소명\" column을 \".\"으로 split했을 때 가장 앞에 나오는 항목을 가져와서 만든다.\n","\n","df_loc_selected = \"fill here\"  # df_loc dataframe에서 \"대여소번호\", \"거치대수\", \"대여소명\", \"위도\" column을 선택한다.\n","\n","df_joined = \"fill here\"  # df_usage_selected dataframe과 df_loc_selected dataframe을 \"대여소번호\" column에 대해 inner join한다. (두 테이블에 모두 존재하는 대여소만 사용하기 위함)\n","\n","result = df_joined.\"fill here\"  # \"대여소명\" column으로 grouping한다.\n","\n","result = result.\"fill here\"  # \"대여소명\" column의 값이 같은 항목들에 대해 \"대여건수\" column의 합을 구해 \"총대여건수\"라는 이름의 column을 만들고, \"거치대수\" column의 평균을 구해 \"평균거치대수\"라는 이름의 column을 만든다.\n","\n","result = result.\"fill here\"  # \"총대여건수\" column을 \"평균거치대수\" column으로 나눈 \"대여비율\"이라는 이름의 column을 만든다.\n","\n","result = result.\"fill here\"  # \"대여비율\" column에 대해 내림차순 정렬한다.\n","\n","result.show(1)"],"metadata":{"id":"xqZYqtiSpfky"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Pandas API on Spark"],"metadata":{"id":"Yfr9F_ikxcUI"}},{"cell_type":"code","source":["import pandas as pd\n","import pyspark.pandas as ps\n","from IPython.display import display # for just pretty print\n","\n","titanic_pd = pd.read_csv('/gdrive/MyDrive/Colab Notebooks/titanic_train.csv')\n","display(titanic_pd.head(5))\n","\n","titanic_ps = ps.from_pandas(titanic_pd)\n","display(titanic_ps.head(5))\n","\n","# Is it always possible? No if the source data(titanic_train.csv) is too big to fit in local node."],"metadata":{"id":"RMOu2b8hxxB0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# So, first you need to read the source with RDD or PySpark Dataframe.\n","# This operation is not limited to the local memory size\n","titanic_df = spark.read.option(\"header\", \"true\") \\\n","                  .option(\"inferSchema\", \"true\") \\\n","                  .csv('/gdrive/MyDrive/Colab Notebooks/titanic_train.csv')\n","\n","display(titanic_df.head(5))\n","\n","# Then you can pass the data to Pandas API on Spark.\n","titanic_ps = titanic_df.to_pandas_on_spark()\n","display(titanic_ps.head(5))\n","\n","# You can pass the data to local Pandas DataFrame, after some diet on data.\n","titanic_pd = titanic_ps.to_pandas()\n","display(titanic_pd.head(5))"],"metadata":{"id":"-PblkwgEzwQH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uDtM9JOCz-J8"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1PkieHrx7BZjIHVBaAtE5XrWK20HD2dKx","timestamp":1693460716055}],"collapsed_sections":["NTbNyMFGzada","kkyLrsXLhoeg"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}